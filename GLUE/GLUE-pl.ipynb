{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Finetune ðŸ¤— Transformers Models with PyTorch Lightning âš¡\n",
    "  * https://nbviewer.jupyter.org/github/PyTorchLightning/pytorch-lightning/blob/master/notebooks/04-transformers-text-classification.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-28 23:57:21] [INIT] GLUE-pl.ipynb (on lightn)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning datasets transformers --quiet\n",
    "import os, time\n",
    "os.environ['CURRENT_FILE'] = 'GLUE-pl.ipynb'\n",
    "!date \"+[%F %R:%S] [INIT] $CURRENT_FILE (on $CONDA_DEFAULT_ENV)\"\n",
    "t0 = time.time()\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "class GLUELightningData(pl.LightningDataModule):\n",
    "    task_text_field_map = {\n",
    "        'cola': ['sentence'],\n",
    "        'mrpc': ['sentence1', 'sentence2'],\n",
    "    }\n",
    "    glue_task_num_labels = {\n",
    "        'cola': 2,\n",
    "        'mrpc': 2,\n",
    "    }\n",
    "    loader_columns = [\n",
    "        'datasets_idx',\n",
    "        'input_ids',\n",
    "        'token_type_ids',\n",
    "        'attention_mask',\n",
    "        'start_positions',\n",
    "        'end_positions',\n",
    "        'labels'\n",
    "    ]\n",
    "\n",
    "    def __init__(self,\n",
    "        task_name: str, transformer_name: str, max_seq_length: int = 128,\n",
    "        train_batch_size: int = 32, eval_batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.task_name = task_name\n",
    "        self.transformer_name = transformer_name\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "\n",
    "        self.text_fields = self.task_text_field_map[task_name]\n",
    "        self.num_labels = self.glue_task_num_labels[task_name]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.transformer_name, use_fast=True)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        datasets.load_dataset('glue', self.task_name)\n",
    "        AutoTokenizer.from_pretrained(self.transformer_name, use_fast=True)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.dataset = datasets.load_dataset('glue', self.task_name)\n",
    "        for split in self.dataset.keys():\n",
    "            self.dataset[split] = self.dataset[split].map(self.to_features,\n",
    "                batched=True, remove_columns=['label'])\n",
    "            self.columns = [c for c in self.dataset[split].column_names if c in self.loader_columns]\n",
    "            self.dataset[split].set_format(type=\"torch\", columns=self.columns)\n",
    "        self.eval_splits = [x for x in self.dataset.keys() if 'validation' in x]\n",
    "\n",
    "    def to_features(self, example_batch, indices=None):\n",
    "        if len(self.text_fields) > 1:\n",
    "            texts = list(zip(example_batch[self.text_fields[0]], example_batch[self.text_fields[1]]))\n",
    "        else:\n",
    "            texts = example_batch[self.text_fields[0]]\n",
    "        features = self.tokenizer.batch_encode_plus(texts,\n",
    "            max_length=self.max_seq_length, pad_to_max_length=True, truncation=True)\n",
    "        features['labels'] = example_batch['label']\n",
    "        return features\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset['train'], batch_size=self.train_batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset['validation'], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if len(self.eval_splits) == 1:\n",
    "            return DataLoader(self.dataset['test'], batch_size=self.eval_batch_size)\n",
    "        elif len(self.eval_splits) > 1:\n",
    "            return [DataLoader(self.dataset[x], batch_size=self.eval_batch_size) for x in self.eval_splits]\n",
    "\n",
    "class GLUELightning(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "        transformer_name: str,\n",
    "        num_labels: int,\n",
    "        learning_rate: float = 2e-5,\n",
    "        adam_epsilon: float = 1e-8,\n",
    "        warmup_steps: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        eval_splits: Optional[list] = None,\n",
    "        **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config = AutoConfig.from_pretrained(transformer_name, num_labels=num_labels)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(transformer_name, config=self.config)\n",
    "        self.metric = datasets.load_metric('glue', self.hparams.task_name,\n",
    "            experiment_id=datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        )\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs[0]\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(**batch)\n",
    "        val_loss, logits = outputs[:2]\n",
    "        if self.hparams.num_labels >= 1:\n",
    "            preds = torch.argmax(logits, axis=1)\n",
    "        else:\n",
    "            preds = logits.squeeze()\n",
    "        labels = batch[\"labels\"]\n",
    "        return {'loss': val_loss, \"preds\": preds, \"labels\": labels}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        preds = torch.cat([x['preds'] for x in outputs]).detach().cpu().numpy()\n",
    "        labels = torch.cat([x['labels'] for x in outputs]).detach().cpu().numpy()\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log_dict(self.metric.compute(predictions=preds, references=labels), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage == 'fit':\n",
    "            train_loader = self.train_dataloader()\n",
    "            self.total_steps = (\n",
    "                (len(train_loader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.gpus)))\n",
    "                // self.hparams.accumulate_grad_batches * float(self.hparams.max_epochs)\n",
    "            )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.total_steps\n",
    "        )\n",
    "        scheduler = {\n",
    "            'scheduler': scheduler,\n",
    "            'interval': 'step',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument(\"--learning_rate\", default=2e-5, type=float)\n",
    "        parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float)\n",
    "        parser.add_argument(\"--warmup_steps\", default=0, type=int)\n",
    "        parser.add_argument(\"--weight_decay\", default=0.0, type=float)\n",
    "        return parser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/chris/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "Reusing dataset glue (/home/chris/.cache/huggingface/datasets/glue/mrpc/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "/home/chris/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1767: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                | Params\n",
      "--------------------------------------------------------------\n",
      "0 | model | DistilBertForSequenceClassification | 65 M  \n",
      "/home/chris/anaconda3/envs/lightn/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/chris/anaconda3/envs/lightn/lib/python3.8/site-packages/datasets/arrow_dataset.py:847: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/home/chris/anaconda3/envs/lightn/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/chris/anaconda3/envs/lightn/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/chris/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1510286d64be4b17830ea5f9a9389ead"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d3a8961f3f145cdbe6c9a9a15afd400"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff99c055add542959c9c772d23a8af50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layoutâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "933a91d58f2842f7a754ce29a5f121fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), maxâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1003d93ec67c43b0a4b1186ed8f4f7c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba0a5a7dc9244ef7b8998cd99fcffaa6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28cfd01ccf6c440e885ba58f8518ff7d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), mâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e377e7d320c4a81b1ab4dd2fda666ba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_args(args=None):\n",
    "    parser = ArgumentParser()\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    parser = GLUELightningData.add_argparse_args(parser)\n",
    "    parser = GLUELightning.add_model_specific_args(parser)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "def main(args):\n",
    "    pl.seed_everything(args.seed)\n",
    "    dm = GLUELightningData.from_argparse_args(args)\n",
    "    dm.prepare_data()\n",
    "    dm.setup('fit')\n",
    "    model = GLUELightning(num_labels=dm.num_labels, eval_splits=dm.eval_splits, **vars(args))\n",
    "    trainer = pl.Trainer.from_argparse_args(args)\n",
    "    return dm, model, trainer\n",
    "\n",
    "dm, model, trainer = main(parse_args(\"\"\"--gpus 1 --max_epochs 3\n",
    "    --task_name mrpc --transformer_name distilbert-base-cased\"\"\".split()))\n",
    "trainer.fit(model, dm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-28 23:58:45] [EXIT] GLUE-pl.ipynb (on lightn) (in 83.380s)\r\n"
     ]
    }
   ],
   "source": [
    "os.environ['ELASPED_TIME'] = f\"{time.time() - t0:.3f}s\"\n",
    "!date \"+[%F %R:%S] [EXIT] $CURRENT_FILE (on $CONDA_DEFAULT_ENV) (in $ELASPED_TIME)\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}