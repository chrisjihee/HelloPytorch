{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-22 06:49:50] [INIT] HelloNSMC (using /home/chris/anaconda3/envs/trans2)\r\n"
     ]
    }
   ],
   "source": [
    "!date \"+[%F %R:%S] [INIT] HelloNSMC (using $CONDA_PREFIX)\"\n",
    "import time\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/chris/.local/lib/python3.8/site-packages (4.50.2)\n",
      "Requirement already satisfied: boto3 in /home/chris/.local/lib/python3.8/site-packages (1.16.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (2.22.0)\n",
      "Requirement already satisfied: regex in /home/chris/.local/lib/python3.8/site-packages (2020.10.15)\n",
      "Requirement already satisfied: sentencepiece in /home/chris/.local/lib/python3.8/site-packages (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /home/chris/.local/lib/python3.8/site-packages (0.0.43)\n",
      "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (1.25.8)\n",
      "Requirement already satisfied: pandas in /home/chris/.local/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: pytorch-transformers in /home/chris/.local/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/chris/.local/lib/python3.8/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/chris/.local/lib/python3.8/site-packages (from boto3) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.0 in /home/chris/.local/lib/python3.8/site-packages (from boto3) (1.19.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from sacremoses) (7.0)\n",
      "Requirement already satisfied: joblib in /home/chris/.local/lib/python3.8/site-packages (from sacremoses) (0.17.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/lib/python3/dist-packages (from pandas) (1.17.4)\n",
      "Requirement already satisfied: torch>=1.0.0 in /home/chris/.local/lib/python3.8/site-packages (from pytorch-transformers) (1.6.0)\n",
      "Requirement already satisfied: future in /home/chris/.local/lib/python3.8/site-packages (from torch>=1.0.0->pytorch-transformers) (0.18.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses urllib3 pandas pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded: nsmc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./nsmc/\"):\n",
    "    !git clone https://github.com/e9t/nsmc.git\n",
    "else:\n",
    "    print(\"Dataset already downloaded: nsmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-22 06:49:51] [DONE] Environment & Dataset Installation\n",
      "td=0.893\n"
     ]
    }
   ],
   "source": [
    "!date \"+[%F %R:%S] [DONE] Environment & Dataset Installation\"\n",
    "print(f\"td={time.time() - t0:.3f}\")\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                           document  label\n",
       "0   9976970                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                  너무재밓었다그래서보는것을추천한다      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./nsmc/ratings_train.txt', sep='\\t')\n",
    "test_df = pd.read_csv('./nsmc/ratings_test.txt', sep='\\t')\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "train_df = train_df.sample(frac=0.4, random_state=999)\n",
    "test_df = test_df.sample(frac=0.4, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NsmcDataset(Dataset):\n",
    "    \"\"\" Naver Sentiment Movie Corpus Dataset \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, 1]\n",
    "        label = self.df.iloc[idx, 2]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nsmc_train_dataset = NsmcDataset(train_df)\n",
    "train_loader = DataLoader(nsmc_train_dataset, batch_size=2, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-22 06:50:05] [DONE] Pretrained Model Loading\n",
      "td=14.165\n"
     ]
    }
   ],
   "source": [
    "!date \"+[%F %R:%S] [DONE] Pretrained Model Loading\"\n",
    "print(f\"td={time.time() - t0:.3f}\")\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-a906961c6f91>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(label)\n",
      "<ipython-input-12-a906961c6f91>:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-22 06:51:00] Training...\n",
      "[Epoch 1/2] Iteration 500 -> Train Loss: 0.6982, Accuracy: 0.5060\n",
      "[2020-10-22 06:51:54] Training...\n",
      "[Epoch 1/2] Iteration 1000 -> Train Loss: 0.6963, Accuracy: 0.4940\n",
      "[2020-10-22 06:52:48] Training...\n",
      "[Epoch 1/2] Iteration 1500 -> Train Loss: 0.6928, Accuracy: 0.5060\n",
      "[2020-10-22 06:53:42] Training...\n",
      "[Epoch 1/2] Iteration 2000 -> Train Loss: 0.6909, Accuracy: 0.5490\n",
      "[2020-10-22 06:54:36] Training...\n",
      "[Epoch 1/2] Iteration 2500 -> Train Loss: 0.6773, Accuracy: 0.5790\n",
      "[2020-10-22 06:55:30] Training...\n",
      "[Epoch 1/2] Iteration 3000 -> Train Loss: 0.6357, Accuracy: 0.6560\n",
      "[2020-10-22 06:56:23] Training...\n",
      "[Epoch 1/2] Iteration 3500 -> Train Loss: 0.5870, Accuracy: 0.7040\n",
      "[2020-10-22 06:57:17] Training...\n",
      "[Epoch 1/2] Iteration 4000 -> Train Loss: 0.5770, Accuracy: 0.7090\n",
      "[2020-10-22 06:58:10] Training...\n",
      "[Epoch 1/2] Iteration 4500 -> Train Loss: 0.5526, Accuracy: 0.7270\n",
      "[2020-10-22 06:59:04] Training...\n",
      "[Epoch 1/2] Iteration 5000 -> Train Loss: 0.5482, Accuracy: 0.7270\n",
      "[2020-10-22 06:59:57] Training...\n",
      "[Epoch 1/2] Iteration 5500 -> Train Loss: 0.5652, Accuracy: 0.7110\n",
      "[2020-10-22 07:00:51] Training...\n",
      "[Epoch 1/2] Iteration 6000 -> Train Loss: 0.5225, Accuracy: 0.7480\n",
      "[2020-10-22 07:01:45] Training...\n",
      "[Epoch 1/2] Iteration 6500 -> Train Loss: 0.5163, Accuracy: 0.7530\n",
      "[2020-10-22 07:02:38] Training...\n",
      "[Epoch 1/2] Iteration 7000 -> Train Loss: 0.5247, Accuracy: 0.7420\n",
      "[2020-10-22 07:03:32] Training...\n",
      "[Epoch 1/2] Iteration 7500 -> Train Loss: 0.5131, Accuracy: 0.7480\n",
      "[2020-10-22 07:04:26] Training...\n",
      "[Epoch 1/2] Iteration 8000 -> Train Loss: 0.5159, Accuracy: 0.7530\n",
      "[2020-10-22 07:05:19] Training...\n",
      "[Epoch 1/2] Iteration 8500 -> Train Loss: 0.4919, Accuracy: 0.7580\n",
      "[2020-10-22 07:06:13] Training...\n",
      "[Epoch 1/2] Iteration 9000 -> Train Loss: 0.5097, Accuracy: 0.7520\n",
      "[2020-10-22 07:07:07] Training...\n",
      "[Epoch 1/2] Iteration 9500 -> Train Loss: 0.5137, Accuracy: 0.7460\n",
      "[2020-10-22 07:08:00] Training...\n",
      "[Epoch 1/2] Iteration 10000 -> Train Loss: 0.4820, Accuracy: 0.7680\n",
      "[2020-10-22 07:08:54] Training...\n",
      "[Epoch 1/2] Iteration 10500 -> Train Loss: 0.5027, Accuracy: 0.7500\n",
      "[2020-10-22 07:09:48] Training...\n",
      "[Epoch 1/2] Iteration 11000 -> Train Loss: 0.4663, Accuracy: 0.7760\n",
      "[2020-10-22 07:10:41] Training...\n",
      "[Epoch 1/2] Iteration 11500 -> Train Loss: 0.4605, Accuracy: 0.7760\n",
      "[2020-10-22 07:11:35] Training...\n",
      "[Epoch 1/2] Iteration 12000 -> Train Loss: 0.4734, Accuracy: 0.7600\n",
      "[2020-10-22 07:12:29] Training...\n",
      "[Epoch 1/2] Iteration 12500 -> Train Loss: 0.4826, Accuracy: 0.7530\n",
      "[2020-10-22 07:13:23] Training...\n",
      "[Epoch 1/2] Iteration 13000 -> Train Loss: 0.4699, Accuracy: 0.7760\n",
      "[2020-10-22 07:14:17] Training...\n",
      "[Epoch 1/2] Iteration 13500 -> Train Loss: 0.4808, Accuracy: 0.7600\n",
      "[2020-10-22 07:15:11] Training...\n",
      "[Epoch 1/2] Iteration 14000 -> Train Loss: 0.4533, Accuracy: 0.7810\n",
      "[2020-10-22 07:16:04] Training...\n",
      "[Epoch 1/2] Iteration 14500 -> Train Loss: 0.4390, Accuracy: 0.7850\n",
      "[2020-10-22 07:16:58] Training...\n",
      "[Epoch 1/2] Iteration 15000 -> Train Loss: 0.4679, Accuracy: 0.7820\n",
      "[2020-10-22 07:17:51] Training...\n",
      "[Epoch 1/2] Iteration 15500 -> Train Loss: 0.4398, Accuracy: 0.8080\n",
      "[2020-10-22 07:18:45] Training...\n",
      "[Epoch 1/2] Iteration 16000 -> Train Loss: 0.4529, Accuracy: 0.7860\n",
      "[2020-10-22 07:19:39] Training...\n",
      "[Epoch 1/2] Iteration 16500 -> Train Loss: 0.4164, Accuracy: 0.8170\n",
      "[2020-10-22 07:20:33] Training...\n",
      "[Epoch 1/2] Iteration 17000 -> Train Loss: 0.4311, Accuracy: 0.8090\n",
      "[2020-10-22 07:21:27] Training...\n",
      "[Epoch 1/2] Iteration 17500 -> Train Loss: 0.4433, Accuracy: 0.7930\n",
      "[2020-10-22 07:22:21] Training...\n",
      "[Epoch 1/2] Iteration 18000 -> Train Loss: 0.4353, Accuracy: 0.7960\n",
      "[2020-10-22 07:23:15] Training...\n",
      "[Epoch 1/2] Iteration 18500 -> Train Loss: 0.4234, Accuracy: 0.8060\n",
      "[2020-10-22 07:24:09] Training...\n",
      "[Epoch 1/2] Iteration 19000 -> Train Loss: 0.4314, Accuracy: 0.8060\n",
      "[2020-10-22 07:25:02] Training...\n",
      "[Epoch 1/2] Iteration 19500 -> Train Loss: 0.4439, Accuracy: 0.7970\n",
      "[2020-10-22 07:25:56] Training...\n",
      "[Epoch 1/2] Iteration 20000 -> Train Loss: 0.4105, Accuracy: 0.8070\n",
      "[2020-10-22 07:26:50] Training...\n",
      "[Epoch 1/2] Iteration 20500 -> Train Loss: 0.4358, Accuracy: 0.8100\n",
      "[2020-10-22 07:27:44] Training...\n",
      "[Epoch 1/2] Iteration 21000 -> Train Loss: 0.4305, Accuracy: 0.8040\n",
      "[2020-10-22 07:28:38] Training...\n",
      "[Epoch 1/2] Iteration 21500 -> Train Loss: 0.4204, Accuracy: 0.8090\n",
      "[2020-10-22 07:29:32] Training...\n",
      "[Epoch 1/2] Iteration 22000 -> Train Loss: 0.4484, Accuracy: 0.7820\n",
      "[2020-10-22 07:30:26] Training...\n",
      "[Epoch 1/2] Iteration 22500 -> Train Loss: 0.4478, Accuracy: 0.7840\n",
      "[2020-10-22 07:31:19] Training...\n",
      "[Epoch 1/2] Iteration 23000 -> Train Loss: 0.4367, Accuracy: 0.7890\n",
      "[2020-10-22 07:32:13] Training...\n",
      "[Epoch 1/2] Iteration 23500 -> Train Loss: 0.4141, Accuracy: 0.7890\n",
      "[2020-10-22 07:33:07] Training...\n",
      "[Epoch 1/2] Iteration 24000 -> Train Loss: 0.4430, Accuracy: 0.7790\n",
      "[2020-10-22 07:34:01] Training...\n",
      "[Epoch 1/2] Iteration 24500 -> Train Loss: 0.4141, Accuracy: 0.8210\n",
      "[2020-10-22 07:34:54] Training...\n",
      "[Epoch 1/2] Iteration 25000 -> Train Loss: 0.4211, Accuracy: 0.8030\n",
      "[2020-10-22 07:35:49] Training...\n",
      "[Epoch 1/2] Iteration 25500 -> Train Loss: 0.4043, Accuracy: 0.8090\n",
      "[2020-10-22 07:36:42] Training...\n",
      "[Epoch 1/2] Iteration 26000 -> Train Loss: 0.4123, Accuracy: 0.8140\n",
      "[2020-10-22 07:37:36] Training...\n",
      "[Epoch 1/2] Iteration 26500 -> Train Loss: 0.4143, Accuracy: 0.7960\n",
      "[2020-10-22 07:38:30] Training...\n",
      "[Epoch 1/2] Iteration 27000 -> Train Loss: 0.4399, Accuracy: 0.7930\n",
      "[2020-10-22 07:39:24] Training...\n",
      "[Epoch 1/2] Iteration 27500 -> Train Loss: 0.3950, Accuracy: 0.8160\n",
      "[2020-10-22 07:40:18] Training...\n",
      "[Epoch 1/2] Iteration 28000 -> Train Loss: 0.4218, Accuracy: 0.8070\n",
      "[2020-10-22 07:41:11] Training...\n",
      "[Epoch 1/2] Iteration 28500 -> Train Loss: 0.4059, Accuracy: 0.8110\n",
      "[2020-10-22 07:42:05] Training...\n",
      "[Epoch 1/2] Iteration 29000 -> Train Loss: 0.3960, Accuracy: 0.8370\n",
      "[2020-10-22 07:42:59] Training...\n",
      "[Epoch 1/2] Iteration 29500 -> Train Loss: 0.3981, Accuracy: 0.8090\n",
      "[2020-10-22 07:43:53] Training...\n",
      "[Epoch 2/2] Iteration 30000 -> Train Loss: 0.3985, Accuracy: 0.8160\n",
      "[2020-10-22 07:44:47] Training...\n",
      "[Epoch 2/2] Iteration 30500 -> Train Loss: 0.4067, Accuracy: 0.8160\n",
      "[2020-10-22 07:45:41] Training...\n",
      "[Epoch 2/2] Iteration 31000 -> Train Loss: 0.3810, Accuracy: 0.8320\n",
      "[2020-10-22 07:46:35] Training...\n",
      "[Epoch 2/2] Iteration 31500 -> Train Loss: 0.3761, Accuracy: 0.8330\n",
      "[2020-10-22 07:47:28] Training...\n",
      "[Epoch 2/2] Iteration 32000 -> Train Loss: 0.4112, Accuracy: 0.8140\n",
      "[2020-10-22 07:48:22] Training...\n",
      "[Epoch 2/2] Iteration 32500 -> Train Loss: 0.3601, Accuracy: 0.8310\n",
      "[2020-10-22 07:49:17] Training...\n",
      "[Epoch 2/2] Iteration 33000 -> Train Loss: 0.3848, Accuracy: 0.8320\n",
      "[2020-10-22 07:50:10] Training...\n",
      "[Epoch 2/2] Iteration 33500 -> Train Loss: 0.3911, Accuracy: 0.8170\n",
      "[2020-10-22 07:51:04] Training...\n",
      "[Epoch 2/2] Iteration 34000 -> Train Loss: 0.3519, Accuracy: 0.8370\n",
      "[2020-10-22 07:51:58] Training...\n",
      "[Epoch 2/2] Iteration 34500 -> Train Loss: 0.3855, Accuracy: 0.8380\n",
      "[2020-10-22 07:52:51] Training...\n",
      "[Epoch 2/2] Iteration 35000 -> Train Loss: 0.3850, Accuracy: 0.8280\n",
      "[2020-10-22 07:53:45] Training...\n",
      "[Epoch 2/2] Iteration 35500 -> Train Loss: 0.3618, Accuracy: 0.8300\n",
      "[2020-10-22 07:54:39] Training...\n",
      "[Epoch 2/2] Iteration 36000 -> Train Loss: 0.3859, Accuracy: 0.8140\n",
      "[2020-10-22 07:55:32] Training...\n",
      "[Epoch 2/2] Iteration 36500 -> Train Loss: 0.3798, Accuracy: 0.8270\n",
      "[2020-10-22 07:56:26] Training...\n",
      "[Epoch 2/2] Iteration 37000 -> Train Loss: 0.4029, Accuracy: 0.8190\n",
      "[2020-10-22 07:57:20] Training...\n",
      "[Epoch 2/2] Iteration 37500 -> Train Loss: 0.3740, Accuracy: 0.8200\n",
      "[2020-10-22 07:58:14] Training...\n",
      "[Epoch 2/2] Iteration 38000 -> Train Loss: 0.3464, Accuracy: 0.8540\n",
      "[2020-10-22 07:59:08] Training...\n",
      "[Epoch 2/2] Iteration 38500 -> Train Loss: 0.3727, Accuracy: 0.8270\n",
      "[2020-10-22 08:00:01] Training...\n",
      "[Epoch 2/2] Iteration 39000 -> Train Loss: 0.4070, Accuracy: 0.8150\n",
      "[2020-10-22 08:00:55] Training...\n",
      "[Epoch 2/2] Iteration 39500 -> Train Loss: 0.3852, Accuracy: 0.8270\n",
      "[2020-10-22 08:01:49] Training...\n",
      "[Epoch 2/2] Iteration 40000 -> Train Loss: 0.3942, Accuracy: 0.8240\n",
      "[2020-10-22 08:02:42] Training...\n",
      "[Epoch 2/2] Iteration 40500 -> Train Loss: 0.3907, Accuracy: 0.8250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-22 08:03:36] Training...\n",
      "[Epoch 2/2] Iteration 41000 -> Train Loss: 0.3383, Accuracy: 0.8450\n",
      "[2020-10-22 08:04:29] Training...\n",
      "[Epoch 2/2] Iteration 41500 -> Train Loss: 0.3899, Accuracy: 0.8180\n",
      "[2020-10-22 08:05:23] Training...\n",
      "[Epoch 2/2] Iteration 42000 -> Train Loss: 0.3927, Accuracy: 0.8290\n",
      "[2020-10-22 08:06:16] Training...\n",
      "[Epoch 2/2] Iteration 42500 -> Train Loss: 0.3821, Accuracy: 0.8380\n",
      "[2020-10-22 08:07:10] Training...\n",
      "[Epoch 2/2] Iteration 43000 -> Train Loss: 0.3852, Accuracy: 0.8350\n",
      "[2020-10-22 08:08:03] Training...\n",
      "[Epoch 2/2] Iteration 43500 -> Train Loss: 0.3946, Accuracy: 0.8200\n",
      "[2020-10-22 08:08:57] Training...\n",
      "[Epoch 2/2] Iteration 44000 -> Train Loss: 0.3757, Accuracy: 0.8440\n",
      "[2020-10-22 08:09:51] Training...\n",
      "[Epoch 2/2] Iteration 44500 -> Train Loss: 0.3902, Accuracy: 0.8200\n",
      "[2020-10-22 08:10:45] Training...\n",
      "[Epoch 2/2] Iteration 45000 -> Train Loss: 0.3725, Accuracy: 0.8330\n",
      "[2020-10-22 08:11:39] Training...\n",
      "[Epoch 2/2] Iteration 45500 -> Train Loss: 0.3719, Accuracy: 0.8300\n",
      "[2020-10-22 08:12:33] Training...\n",
      "[Epoch 2/2] Iteration 46000 -> Train Loss: 0.3832, Accuracy: 0.8310\n",
      "[2020-10-22 08:13:26] Training...\n",
      "[Epoch 2/2] Iteration 46500 -> Train Loss: 0.3591, Accuracy: 0.8420\n",
      "[2020-10-22 08:14:20] Training...\n",
      "[Epoch 2/2] Iteration 47000 -> Train Loss: 0.3680, Accuracy: 0.8390\n",
      "[2020-10-22 08:15:13] Training...\n",
      "[Epoch 2/2] Iteration 47500 -> Train Loss: 0.3506, Accuracy: 0.8630\n",
      "[2020-10-22 08:16:07] Training...\n",
      "[Epoch 2/2] Iteration 48000 -> Train Loss: 0.3762, Accuracy: 0.8300\n",
      "[2020-10-22 08:17:00] Training...\n",
      "[Epoch 2/2] Iteration 48500 -> Train Loss: 0.3723, Accuracy: 0.8280\n",
      "[2020-10-22 08:17:54] Training...\n",
      "[Epoch 2/2] Iteration 49000 -> Train Loss: 0.3756, Accuracy: 0.8370\n",
      "[2020-10-22 08:18:48] Training...\n",
      "[Epoch 2/2] Iteration 49500 -> Train Loss: 0.3164, Accuracy: 0.8690\n",
      "[2020-10-22 08:19:42] Training...\n",
      "[Epoch 2/2] Iteration 50000 -> Train Loss: 0.3760, Accuracy: 0.8240\n",
      "[2020-10-22 08:20:35] Training...\n",
      "[Epoch 2/2] Iteration 50500 -> Train Loss: 0.3918, Accuracy: 0.8140\n",
      "[2020-10-22 08:21:29] Training...\n",
      "[Epoch 2/2] Iteration 51000 -> Train Loss: 0.3667, Accuracy: 0.8420\n",
      "[2020-10-22 08:22:23] Training...\n",
      "[Epoch 2/2] Iteration 51500 -> Train Loss: 0.3558, Accuracy: 0.8450\n",
      "[2020-10-22 08:23:17] Training...\n",
      "[Epoch 2/2] Iteration 52000 -> Train Loss: 0.3445, Accuracy: 0.8420\n",
      "[2020-10-22 08:24:11] Training...\n",
      "[Epoch 2/2] Iteration 52500 -> Train Loss: 0.3636, Accuracy: 0.8340\n",
      "[2020-10-22 08:25:05] Training...\n",
      "[Epoch 2/2] Iteration 53000 -> Train Loss: 0.3626, Accuracy: 0.8390\n",
      "[2020-10-22 08:25:58] Training...\n",
      "[Epoch 2/2] Iteration 53500 -> Train Loss: 0.3618, Accuracy: 0.8400\n",
      "[2020-10-22 08:26:52] Training...\n",
      "[Epoch 2/2] Iteration 54000 -> Train Loss: 0.3485, Accuracy: 0.8360\n",
      "[2020-10-22 08:27:45] Training...\n",
      "[Epoch 2/2] Iteration 54500 -> Train Loss: 0.3587, Accuracy: 0.8420\n",
      "[2020-10-22 08:28:39] Training...\n",
      "[Epoch 2/2] Iteration 55000 -> Train Loss: 0.3635, Accuracy: 0.8350\n",
      "[2020-10-22 08:29:33] Training...\n",
      "[Epoch 2/2] Iteration 55500 -> Train Loss: 0.3702, Accuracy: 0.8210\n",
      "[2020-10-22 08:30:26] Training...\n",
      "[Epoch 2/2] Iteration 56000 -> Train Loss: 0.3679, Accuracy: 0.8360\n",
      "[2020-10-22 08:31:20] Training...\n",
      "[Epoch 2/2] Iteration 56500 -> Train Loss: 0.3702, Accuracy: 0.8390\n",
      "[2020-10-22 08:32:14] Training...\n",
      "[Epoch 2/2] Iteration 57000 -> Train Loss: 0.3586, Accuracy: 0.8510\n",
      "[2020-10-22 08:33:08] Training...\n",
      "[Epoch 2/2] Iteration 57500 -> Train Loss: 0.3625, Accuracy: 0.8450\n",
      "[2020-10-22 08:34:02] Training...\n",
      "[Epoch 2/2] Iteration 58000 -> Train Loss: 0.3471, Accuracy: 0.8480\n",
      "[2020-10-22 08:34:55] Training...\n",
      "[Epoch 2/2] Iteration 58500 -> Train Loss: 0.3588, Accuracy: 0.8240\n",
      "[2020-10-22 08:35:49] Training...\n",
      "[Epoch 2/2] Iteration 59000 -> Train Loss: 0.3895, Accuracy: 0.8280\n",
      "[2020-10-22 08:36:43] Training...\n",
      "[Epoch 2/2] Iteration 59500 -> Train Loss: 0.3714, Accuracy: 0.8260\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "epochs = 2\n",
    "itr = 1\n",
    "p_itr = 500\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for text, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
    "        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        labels = torch.tensor(label)\n",
    "        outputs = model(sample, labels=labels)\n",
    "        loss, logits = outputs\n",
    "\n",
    "        pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "        correct = pred.eq(labels)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % p_itr == 0:\n",
    "            !date \"+[%F %R:%S] Training...\"\n",
    "            print(f\"[Epoch {epoch+1}/{epochs}] Iteration {itr} -> Train Loss: {total_loss/p_itr:.4f}, Accuracy: {total_correct/total_len:.4f}\")\n",
    "            total_loss = 0\n",
    "            total_len = 0\n",
    "            total_correct = 0\n",
    "\n",
    "        itr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-22 08:37:36] [DONE] Model Fine-tuning\n",
      "td=6450.898\n"
     ]
    }
   ],
   "source": [
    "!date \"+[%F %R:%S] [DONE] Model Fine-tuning\"\n",
    "print(f\"td={time.time() - t0:.3f}\")\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-36dad375e840>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(label)\n",
      "<ipython-input-14-36dad375e840>:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = torch.argmax(F.softmax(logits), dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8374\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "model.eval()\n",
    "\n",
    "nsmc_eval_dataset = NsmcDataset(test_df)\n",
    "eval_loader = DataLoader(nsmc_eval_dataset, batch_size=2, shuffle=False, num_workers=2)\n",
    "\n",
    "total_loss = 0\n",
    "total_len = 0\n",
    "total_correct = 0\n",
    "\n",
    "for text, label in eval_loader:\n",
    "    encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
    "    padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
    "    sample = torch.tensor(padded_list)\n",
    "    sample, label = sample.to(device), label.to(device)\n",
    "    labels = torch.tensor(label)\n",
    "    outputs = model(sample, labels=labels)\n",
    "    _, logits = outputs\n",
    "\n",
    "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "    correct = pred.eq(labels)\n",
    "    total_correct += correct.sum().item()\n",
    "    total_len += len(labels)\n",
    "\n",
    "print(f\"Test accuracy: {total_correct / total_len:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-22 08:42:03] [EXIT] HelloNSMC (using /home/chris/anaconda3/envs/trans2)\n",
      "td=266.871\n"
     ]
    }
   ],
   "source": [
    "!date \"+[%F %R:%S] [EXIT] HelloNSMC (using $CONDA_PREFIX)\"\n",
    "print(f\"td={time.time() - t0:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
