{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Pytorch Lightning ⚡\n",
    "  * https://nbviewer.jupyter.org/github/PyTorchLightning/pytorch-lightning/blob/master/notebooks/01-mnist-hello-world.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-29 14:32:09] [INIT] MNIST-with-lightning.ipynb (on lightn)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning --quiet\n",
    "\n",
    "import os\n",
    "os.environ['CURRENT_FILE'] = 'MNIST-with-lightning.ipynb'\n",
    "!date \"+[%F %R:%S] [INIT] $CURRENT_FILE (on $CONDA_DEFAULT_ENV)\"\n",
    "\n",
    "import time\n",
    "t0 = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import Trainer, LightningModule, LightningDataModule\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "def str_loss(loss: List[Tensor]):\n",
    "    metric = torch.mean(torch.stack(loss))\n",
    "    return f'{metric:.4f}'\n",
    "\n",
    "\n",
    "def str_accuracy(acc: Accuracy, detail: bool = False):\n",
    "    backup = acc.correct, acc.total\n",
    "    metric = acc.compute()\n",
    "    acc.correct, acc.total = backup\n",
    "    return f'{metric * 100:.2f}%' if not detail else f'{metric * 100:.2f}%(={acc.correct}/{acc.total})'\n",
    "\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "\n",
    "class DataMNIST(LightningDataModule):\n",
    "    def __init__(self, data_dir: str = '/dat/data/', batch_size: int = 100, num_workers: int = 8):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transforms.ToTensor()\n",
    "        self.dataset = dict()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset['train'], self.dataset['valid'] = random_split(MNIST(self.data_dir, train=True, transform=self.transform), [55000, 5000])\n",
    "        self.dataset['test'] = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset['train'], batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset['valid'], batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset['test'], batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "class ModelMNIST(LightningModule):\n",
    "    def __init__(self, learning_rate, metric_detail=True):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.metric_detail = metric_detail\n",
    "        self.metric = {\n",
    "            'train': {\"loss\": list(), \"acc\": Accuracy()},\n",
    "            'valid': {\"loss\": list(), \"acc\": Accuracy()},\n",
    "            'test': {\"loss\": list(), \"acc\": Accuracy()},\n",
    "        }\n",
    "\n",
    "        self.conv1A = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1A = nn.ReLU()\n",
    "        self.conv1B = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1B = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2A = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2A = nn.ReLU()\n",
    "        self.conv2B = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2B = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3A = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3A = nn.ReLU()\n",
    "        self.conv3B = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3B = nn.ReLU()\n",
    "\n",
    "        self.fc = nn.Linear(7 * 7 * 128, 10, bias=True)\n",
    "        self.fc_bn = nn.BatchNorm1d(10)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.conv1A(inp)\n",
    "        out = self.relu1A(out)\n",
    "        out = self.conv1B(out)\n",
    "        out = self.relu1B(out)\n",
    "        out = self.pool1(out)\n",
    "\n",
    "        out = self.conv2A(out)\n",
    "        out = self.relu2A(out)\n",
    "        out = self.conv2B(out)\n",
    "        out = self.relu2B(out)\n",
    "        out = self.pool2(out)\n",
    "\n",
    "        out = self.conv3A(out)\n",
    "        out = self.relu3A(out)\n",
    "        out = self.conv3B(out)\n",
    "        out = self.relu3B(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc_bn(out)\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch: List[Tensor], batch_idx: int):\n",
    "        inputs: Tensor = batch[0]\n",
    "        labels: Tensor = batch[1]\n",
    "        logits: Tensor = self(inputs)\n",
    "        loss = cross_entropy(logits, labels)\n",
    "        logits = logits.detach().cpu()\n",
    "        labels = labels.detach().cpu()\n",
    "        self.metric['train']['acc'].update(preds=logits, target=labels)\n",
    "        self.metric['train']['loss'].append(loss.detach().cpu())\n",
    "        return {'loss': loss, 'logits': logits, 'labels': labels}\n",
    "\n",
    "    def validation_step(self, batch: List[Tensor], batch_idx: int):\n",
    "        inputs: Tensor = batch[0]\n",
    "        labels: Tensor = batch[1]\n",
    "        logits: Tensor = self(inputs)\n",
    "        loss = cross_entropy(logits, labels)\n",
    "        logits = logits.detach().cpu()\n",
    "        labels = labels.detach().cpu()\n",
    "        self.metric['valid']['acc'].update(preds=logits, target=labels)\n",
    "        self.metric['valid']['loss'].append(loss.detach().cpu())\n",
    "        return {'loss': loss, 'logits': logits, 'labels': labels}\n",
    "\n",
    "    def test_step(self, batch: List[Tensor], batch_idx: int):\n",
    "        inputs: Tensor = batch[0]\n",
    "        labels: Tensor = batch[1]\n",
    "        logits: Tensor = self(inputs)\n",
    "        loss = cross_entropy(logits, labels)\n",
    "        logits = logits.detach().cpu()\n",
    "        labels = labels.detach().cpu()\n",
    "        self.metric['test']['acc'].update(preds=logits, target=labels)\n",
    "        self.metric['test']['loss'].append(loss.detach().cpu())\n",
    "        return {'loss': loss, 'logits': logits, 'labels': labels}\n",
    "\n",
    "    def test_epoch_end(self, outputs: List[Any]):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        for k in self.metric.keys():\n",
    "            self.metric[k]['loss'] = list()\n",
    "            self.metric[k]['acc'].reset()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        print()\n",
    "        print(f\"| Loss     | {{\"\n",
    "              f\" train: {str_loss(self.metric['train']['loss'])},\"\n",
    "              f\" valid: {str_loss(self.metric['valid']['loss'])} }}\")\n",
    "        print(f\"| Accuracy | {{\"\n",
    "              f\" train: {str_accuracy(self.metric['train']['acc'], self.metric_detail)},\"\n",
    "              f\" valid: {str_accuracy(self.metric['valid']['acc'], self.metric_detail)} }}\")\n",
    "        print(\"=\" * 5 + f\" [DONE] [Epoch {self.current_epoch + 1}/{self.trainer.max_epochs}] \" + \"=\" * 70)\n",
    "        print()\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        print()\n",
    "        print(f\"| Loss     | {{\"\n",
    "              f\" test: {str_loss(self.metric['test']['loss'])},\"\n",
    "              f\" valid: {str_loss(self.metric['valid']['loss'])} }}\")\n",
    "        print(f\"| Accuracy | {{\"\n",
    "              f\" test: {str_accuracy(self.metric['test']['acc'], self.metric_detail)},\"\n",
    "              f\" valid: {str_accuracy(self.metric['valid']['acc'], self.metric_detail)} }}\")\n",
    "        print(\"=\" * 5 + f\" [DONE] [Test Epoch] \" + \"=\" * 70)\n",
    "        print()\n",
    "\n",
    "\n",
    "trainer = Trainer(max_epochs=5, num_sanity_val_steps=0, progress_bar_refresh_rate=20, gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name   | Type        | Params\n",
      "----------------------------------------\n",
      "0  | conv1A | Conv2d      | 320   \n",
      "1  | relu1A | ReLU        | 0     \n",
      "2  | conv1B | Conv2d      | 9 K   \n",
      "3  | relu1B | ReLU        | 0     \n",
      "4  | pool1  | MaxPool2d   | 0     \n",
      "5  | conv2A | Conv2d      | 18 K  \n",
      "6  | relu2A | ReLU        | 0     \n",
      "7  | conv2B | Conv2d      | 36 K  \n",
      "8  | relu2B | ReLU        | 0     \n",
      "9  | pool2  | MaxPool2d   | 0     \n",
      "10 | conv3A | Conv2d      | 73 K  \n",
      "11 | relu3A | ReLU        | 0     \n",
      "12 | conv3B | Conv2d      | 147 K \n",
      "13 | relu3B | ReLU        | 0     \n",
      "14 | fc     | Linear      | 62 K  \n",
      "15 | fc_bn  | BatchNorm1d | 20    \n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7d06b4e04fc4765b25ef38726051cc8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca880a6b9e3b465b92a4627e50fb775d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Loss     | { train: 0.3325, valid: 0.1345 }\n",
      "| Accuracy | { train: 97.26%(=53491/55000), valid: 99.10%(=4955/5000) }\n",
      "===== [DONE] [Epoch 1/5] ======================================================================\n",
      "\n",
      "\n",
      "| Loss     | { train: 0.1270, valid: 0.0816 }\n",
      "| Accuracy | { train: 99.11%(=54511/55000), valid: 99.36%(=4968/5000) }\n",
      "===== [DONE] [Epoch 2/5] ======================================================================\n",
      "\n",
      "\n",
      "| Loss     | { train: 0.0731, valid: 0.0550 }\n",
      "| Accuracy | { train: 99.41%(=54674/55000), valid: 99.32%(=4966/5000) }\n",
      "===== [DONE] [Epoch 3/5] ======================================================================\n",
      "\n",
      "\n",
      "| Loss     | { train: 0.0466, valid: 0.0380 }\n",
      "| Accuracy | { train: 99.63%(=54796/55000), valid: 99.34%(=4967/5000) }\n",
      "===== [DONE] [Epoch 4/5] ======================================================================\n",
      "\n",
      "\n",
      "| Loss     | { train: 0.0315, valid: 0.0342 }\n",
      "| Accuracy | { train: 99.78%(=54879/55000), valid: 99.22%(=4961/5000) }\n",
      "===== [DONE] [Epoch 5/5] ======================================================================\n",
      "\n",
      "\n",
      "* Train Time: 34.975s\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "162b25ac702e46d589447541a4b48754"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aaa8ad55990348f0b2b07cce0df13c55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b9316e74af443f48e9ccab284f318a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9637ac53734640de833f4e831622f1aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = time.time()\n",
    "trainer.fit(model=ModelMNIST(learning_rate=0.001), datamodule=DataMNIST())\n",
    "print(f\"* Train Time: {time.time() - t:.3f}s\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f71f3e4c0a64adeb93face80f3f5096"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "| Loss     | { test: 0.0285, valid: 0.0342 }\n",
      "| Accuracy | { test: 99.42%(=9942/10000), valid: 99.22%(=4961/5000) }\n",
      "===== [DONE] [Test Epoch] ======================================================================\n",
      "\n",
      "\n",
      "* Test Time: 0.742s\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "trainer.test()\n",
    "print(f\"* Test Time: {time.time() - t:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-10-29 14:32:47] [EXIT] MNIST-with-lightning.ipynb (on lightn) (in 38.140s)\r\n"
     ]
    }
   ],
   "source": [
    "os.environ['ELASPED_TIME'] = f\"{time.time() - t0:.3f}s\"\n",
    "!date \"+[%F %R:%S] [EXIT] $CURRENT_FILE (on $CONDA_DEFAULT_ENV) (in $ELASPED_TIME)\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}